\documentclass[12pt]{article}
\usepackage{setspace}       % For setting spacing
\usepackage{titlesec}       % For customizing title styles
\usepackage{float}
\usepackage{tocloft}        % For customizing the table of contents
\usepackage{times}          % Use Times New Roman font
\usepackage[utf8]{inputenc} % Input encoding
\usepackage[margin=1in]{geometry} % Set margins
\usepackage{apacite}        % APA-style references
\usepackage{url}            % For formatting URLs
\usepackage{tikz}
\usepackage{enumitem}
\usepackage{listings} % For syntax-highlighted code blocks
\usepackage{xcolor}
\usepackage{graphicx}
\usetikzlibrary{arrows.meta, shapes.geometric}

% Set 1.5 line spacing
\renewcommand{\baselinestretch}{1.5} 

% Title format
\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{1em}{}

\lstset{
  basicstyle=\ttfamily\small, % Code font and size
  keywordstyle=\color{blue},  % Keywords in blue
  stringstyle=\color{red},    % Strings in red
  commentstyle=\color{gray},  % Comments in gray
  numbers=left,               % Line numbers on the left
  numberstyle=\tiny,          % Line number font size
  stepnumber=1,               % Line number step
  frame=single,               % Box around the code
  tabsize=2,                  % Tab size
  breaklines=true,            % Line breaking
  showstringspaces=false      % Do not show spaces in strings
}

\begin{document}

% Cover Page
\begin{titlepage}
    \centering
    \vspace*{2in}
    {\Huge \bfseries Predicting the Winner of Chess Games Using Binary Classificaiton Machine Learning Models \\[2in]}
    {\Large Seth Schwiethale \\[0.5in]}
    {\Large December 2, 2024 \\}
    \vfill
\end{titlepage}

% Table of Contents
\newpage
\tableofcontents
\newpage

% Content Sections
\section{Introduction}
Chess has been one of the world's most popular games for centuries. Over 10 years ago, it was estimated that 605 million adults regularly play chess \cite{fidePressRelease2012}. In the intervening years, the popularity of the game has continued to grow alongside the growth of online gaming where Chess.com alone surpassed 20 million members in 2017 \cite{chessMembers}. With so many chess players around the world, when two meet, how might one predict the winner of the game? This paper compares the application of two Machine Learning binary classification algorithms to this question.

Chess competitions and online platforms commonly use the Elo rating system to indicate the relative skill level of players. Elo ratings can be used to predict the outcome of a match \cite{chessElo}. While Elo ratings are calculated based on the outcomes of previous games, the question is whether a learning algorithm can incorporate additional features of a game to improve upon the prediction accuracy of Elo alone.

To perform this analysis, multiple models were trained using two fundamental supervised classification Machine Learning algorithms: Logistic Regression and Decision Trees. The accuracy of these models were then compared to each other and to the accuracy of predictions made based solely on each player's Elo rating.

\section{Research Background}

\subsection{Elo Ratings and Outcome Prediction}
The Elo rating system was created by Arpad Elo and adopted by the U.S. Chess Federation in 1960. ``Each player's Elo rating is represented by a number that reflects that person's results in previous rated games. After each rated game, their ratings are adjusted according to the outcome of the encounter.'' \cite{chessElo}. Given the Elo rating of each player, the expected outcome can be calculated with the following formula:


\[
E_A = \frac{1}{1 + 10^{(R_B - R_A)/400}}
\]

Where $E_A$ is the expectation that player $A$ will win and $R_A$ and $R_B$ are the Elo ratings of players $A$ and $B$, respectively. An $E_A$ value $> 0.5$ predicts that will win with increasing probability up to $E_A == 1$.

\subsection{Binary Classification}

In Machine Learning, classification ``is a problem of automatcially assigning a label to an unlabeled example'' \cite[Chapter 2, Section 2.7]{100MLB}. If the number of possible labels is two ("White wins", "Black wins"), such a classification problem is called binary classification. Though there are three possible outcomes to a chess game: White wins, Black wins or Draw, this study aims specifically to predict a winner, given some set of features describing the characterstics of a specific game, making it a binary classification problem. It is worth noting that, although about 50\% of games end in a draw amongst the highest-rated players \cite{chessBaseDraws}, most of the world's chess players are not professionals and the occurence of draws is much lower, less than 5\% in the data used in this analysis.

\subsection{Logistic Regression}

Logistic Regression is a supervised binary classification learning alogorithm that uses the standard logistic function, often referred to as the sigmoid function $f_{\textbf{w},b}(\textbf{x})$, whose codomain is $(0, 1)$, to represent the probability that an example is labeled 1. Logistic Regression is trained on a dataset using maximum likelihood optimization by a numerical procedure, such as gradient descent \cite[Chapter 9, Section 9.3]{ShalevShwartz2014}.

\[
f_{\textbf{w},b}(\textbf{x}) := \frac{1}{1 + \exp(-(\langle\textbf{w}, \textbf{x}\rangle + b))}
\]

where $z$ is the inner product of two vectors, the example's features $\textbf{x}$ and the model's learned parameters $\textbf{w}$ plus parameter $b$: $z = \langle\textbf{w}, \textbf{x}\rangle + b$. During training, the log-likelihood $LogL_{\textbf{w},b}$ is maximized for the model's parameters:

\[
LogL_{\textbf{w},b} =
\sum_{i=1}^N y_i\ln{f_{\textbf{w},b}(\textbf{x}) + (1-y_i)\ln(1 - f_{\textbf{w},b}(\textbf{x}))}
\]

\noindent Where $y_i$ is the label for example $i$.

\vspace{12pt}After training, the model has parameters $\textbf{w}$ and $b$ that maximize likelihood that the training dataset is according to the resulting model. These parameters are then used to predict labels for a test set. The results are then used to evaluate the accuracy of the model.

\subsubsection{Considerations}Linear Regression is sensitive to the scale of features. Care must be taken when preprocessing the dataset for training to ensure optimal performance.

\subsubsection{Limitations}Linear Regression is best suited for problems where there is a linear relationship between the features and labels. This may prove to be a limitation when considering the kinds of features available to describe a chess game. Some features, such as opening strategy and game duration may not have a linear relation to the outcomes of the game.

\subsection{Decision Trees}

``A decision tree is an acyclic graph that can be used to make decisions'' \cite[Chapter 3, Section 3.3]{100MLB}. Each branch in a decision tree evaluates a feature in the example and branches left if the value is below a threshold and branches right, otherwise. Decisions trees for classification problems can be constsructed by supervised learning. ``The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features'' \cite{sklearnDT}. Starting with a root node, the data is split according to each feature and all corresponding thresholds for that feature. The feature and threshold whose resulting split has the largest reduction in entropy (largest information gain) is chosen as the branching feature for that node. The process is repeated recusively to construct the rest of the tree until a stopping condition is met.

Decision tree are easy to visually interpret As shown, in Figure~\ref{fig:decision-tree}, and unlike linear regression models, decisiont trees are not sensitive to feature scaling.

\begin{figure}[ht]
\centering
\begin{tikzpicture}[
  edge from parent/.style = {draw, -{Latex}},
  every node/.style = {align=center},
  decision/.style = {draw, diamond, aspect=2, align=center},
  outcome/.style = {draw, circle, align=center},
  level 1/.style = {sibling distance=40mm},
  level 2/.style = {sibling distance=20mm}
]

% Root node
\node {x}
  child {node[decision] {$x_2 < t_1$} % Left child
    child {node[decision] {$x_1 \ge t_2$} % Left child
      child {node[outcome] {A}}
      child {node[outcome] {B}}
    }
    child {node[outcome] {B}}
  };

\end{tikzpicture}
\caption{A simple decision tree demonstrating decision and leaf nodes.}
\label{fig:decision-tree}
\end{figure}

\subsubsection{Considerations}Decision trees can be biased if trained on unbalanced examples. Care must be taken to balance the dateset in preprocessing to reduce bias in the decision tree model.

\subsubsection{Limitations}Decision trees can become complex and are prone to overfitting the training data.

\subsection{Related Work}

Logistic Regression has been used to predict outcomes in many games, such as European-Football \cite{Prasetio2016}, where the authors achieved 69\% accuracy with just a few features and Ice Hockey \cite{Chin2023}, where Logistic Regression outperformed Decision Tree, Support Vector Machine and Artificial Neural Network models used as benchmarks from prior work.

TODO

\section{Materials and Data Sources}

Training and assessment of the chosen model types was performed using the Chess Game Dataset published at kaggle.com \cite{chessDataset}. The datset contains just over 20,000 games that were collected from the open-source online chess game server Lichess.org \cite{lichessOrg}. The dataset contains 16 raw features or columns with no missing values, Table~\ref{tab:features}.

\subsection{Available Features}

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|}
\hline
$\textbf{Feature}$ & Description \\ \hline
$\textbf{Game ID}$ & A unique string to identify a game \\ \hline
$\textbf{Rated (T/F)}$ & True if the game was rated \\ \hline
$\textbf{Start Time}$ & A time stamp indicating when the game began \\ \hline
$\textbf{End Time}$ & A time stamp indicating when the game ended \\ \hline
$\textbf{Number of Turns}$ & How many turns were made before the game ended \\ \hline
$\textbf{Victory Status}$ & A string describing how the game ended \\ \hline
$\textbf{Winner}$ & A string representing the winner of the game by color, if any \\ \hline
$\textbf{Time Increment}$ & A string representing how the game was timed \\ \hline
$\textbf{White Player ID}$ & A string unique to a player \\ \hline
$\textbf{White Player Rating}$ & Elo rating for player \\ \hline
$\textbf{Black Player ID}$ & A string unique to a player \\ \hline
$\textbf{Black Player Rating}$ & Elo rating for player \\ \hline
$\textbf{Moves}$ & All Moves in Standard Chess Notation \\ \hline
$\textbf{Opening Eco}$ & Standardised Code for any given opening strategy \\ \hline
$\textbf{Opening Name}$ & A string representing the opening strategy \\ \hline
$\textbf{Opening Ply}$ & Number of moves in the opening phase \\ \hline
\end{tabular}
\caption{Available features in dataset.}
\label{tab:features}
\end{table}

As detailed below, different data preprocessing steps were performed for each learning algorithm, but in both cases, games with no winner were filtered from the dataset, leaving 19,108 examples. Additionally, the following features were either not included in either training dataset or were transformed:

\begin{itemize}[label={}, leftmargin=0pt]
  \item $\textbf{Drop: id}$ - The Game ID column is a unique identifier and doesnâ€™t provide any predictive value. Including this feature would surely contribute to overfitting, since this unique set of values only exists in the training set.
  \item $\textbf{Drop: increment\_code}$ - This feature was mistakenly interpreted as the change in Elo rating after the game, so it was not included. Further discussion can be found in Section~\ref{sec:discussion}.
  \item $\textbf{Drop: white\_id and black\_id}$ - The specific goal of the research was to evaluate models that can generalize well, without knowing specifically who the player is.
  \item $\textbf{Transform: created\_at and last\_move\_at}$ - The start and end time were decided to be less predictive than the duration of a game, so these two columns were transformed into a new column: $\textbf{duration}$.
  \item $\textbf{Drop: moves}$ - The moves column contains move sequences, which are high-dimensional and computationally expensive to process for simple predictions. Further discussion can be found in Section~\ref{sec:discussion}.
  \item $\textbf{Drop: opening\_name}$ - Redundant interpretation of $\textbf{opening\_eco}$.
  \item $\textbf{Transform: white\_player\_rating and black\_player\_rating}$ - Convert these features to a single feature which is the difference between the two players' ratings: $\textbf{rating\_diff}$. Positive numbers mean white has a higher rating and negative numbers mean black has a higher rating.
\end{itemize}

\section{Research and Methods}
For this research, implementations of logistic regression and decision tree learning algorithms from scikit learn, the Python machine learning library, were used. Before training each model, some preprocessing of data was performed to optimize the performance of the respective learning algorithm. For each algorithm, multiple models were trained with varying feature sets. For decision trees, the hyperparameters were also tuned for optimal performance. Each model was evaluated by the metrics: Accuracy, precision, recall and F1 Score. The most accurate model from each algorithm is compared in Section~\ref{sec:results}.

\subsection{Logistic Regression Model Training}

\subsubsection{Data Preprocessing}

\noindent\textbf{Encoding and Normalizing Numerical Features}

In order to optimize the performance of the logistic regression learner, the following steps were taken to preprocess the dataset before training. Generally, this included: removing columns with no meaningful predictive value for this research's purpose, encoding string-based features to numerical features on a case-by-case basis, normalizing resulting features and checking for multicollinearity amongst remaining features.


\begin{itemize}[label={}, leftmargin=0pt]
  \item $\textbf{Normalize: rating\_diff, turns, duration}$ - Normalize features by scaling between 0 and 1, using Z-Score, to improve model performance.

  \begin{lstlisting}[language=Python, caption={example: normalizing ratings}]
    # create a new column for rating difference
    df['rating_diff'] = df['white_rating'] - df['black_rating']
    # normalize with Z-Score
    # calculate mean and standard deviation for rating_ddiff
    rating_diff_mean = df['rating_diff'].mean()
    rating_diff_std = df['rating_diff'].std()
    # normalize rating_diff
    df['rating_diff_normalized'] = (df['rating_diff'] - rating_diff_mean) / rating_diff_std
  \end{lstlisting}

  \item $\textbf{Encode: opening\_eco}$ - The opening ECO feature represents chess opening strategies coded with a classification system. To utilize this feature in a logistic regression learner, it was decided to convert each code instance to a count of how many times that opening was used in the dataset. This new feature can be thought of as the opening's popularity. The newly encoded opening ECO was finally normalized. Other encodings for this feature can be imagined, such as ranking openings by how often they resulted in a win for white.
\end{itemize}

\noindent\textbf{Reducing Multicollinearity}

Next, the preprocessed features were analyzed for multicollinearity with a correlation matrix, as shown in Figure~\ref{fig:corr-matrix-1}.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{corr-matrix-1.png}
\caption{Initial Correlation Matrix for Logistic Regression Features}
\label{fig:corr-matrix-1}
\end{figure}

The ratings for black and white players were unsurprisingly correlated (in online games, players of similar experience tend to play each other). These features were not used to train the model. The rating difference between players was used, instead.

$\textbf{opening\_eco\_encoded}$ and $\textbf{opening\_ply}$ are also moderately correlated, so PCA was applied to reduce dimensionality in the data set. The principal component of these two features was given the name $\textbf{pca\_opening\_features}$. The resulting correlation matrix is shown in Figure~\ref{fig:corr-matrix-2}.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{corr-matrix-2.png}
\caption{Final Correlation Matrix for Logistic Regression Features}
\label{fig:corr-matrix-2}
\end{figure}

\subsubsection{Training and Evaluation}

The dataset was split into training and testing sets, 80\% and 20\%, respectively. The scikit-learn LogisticRegression implementation was used to fit the models to the training data. Many models were produced, each using a different set of features. Each model was evaluated with the testing set, using the metrics: Accuracy, precision, recall and F1 Score.

\subsection{Decision Tree Model Training}

\subsubsection{Data Preprocessing}
Less preprocessing was required for the decision tree learner than was for the logistic regression learner because decision trees are not sensitive to feature scaling. However, while decision trees can theoretically handle both numerical and categorical data, the scikit-learn implementation does not support catergorical variables \cite{sklearnDT}, so features of interest with categorical representations were encoded with an appropriate numerical representation.

\begin{itemize}[label={}, leftmargin=0pt]
  \item \textbf{Encode: rated} - boolean feature encoded to $False \rightarrow 0$ and $True \rightarrow 1$
  \item \textbf{Encode: winner} - label encoded to $white \rightarrow 0$ and $black \rightarrow 1$
  \item $\textbf{Encode: opening\_eco}$ - The opening ECO feature represents chess opening strategies coded with a classification system. To utilize this feature in a decision tree learner, as in the logistic regression case, it was decided to convert each code instance to a count of how many times that opening was used in the dataset. This new feature can be thought of as the opening's popularity. 
\end{itemize}

\subsubsection{Training and Evaluation}
The dataset was split into training and testing sets, 80\% and 20\%, respectively. The scikit-learn DecisionTree implementation was used to fit the models to the training data. Many models were produced, each using a different set of features. Additionally, the scikit-learn's $GridSearchCV$ \cite{sklearnGSCV} algorithm was used to perform an exhaustive search for optimum hyperparameters: $max\_depth$, $min\_samples\_split$, $min\_samples\_leaf$ and $max\_features$. Each model was evaluated with the testing set, using the metrics: Accuracy, precision, recall and F1 Score.


\section{Results}
\label{sec:results}

\subsection{Predicting with Elo Rating as Baseline}
This is where the results go.

\subsection{Logistic Regression}

\subsection{Decision Trees}

\section{Discussion and Conclusions}
\label{sec:discussion}

\subsection{Limitations to approach}

\subsubsection{Features Used Entail the Game has Ended}

\subsubsection{Correlation of Draw Occurence and Player Rating}

\subsection{Future Work}

\subsubsection{Additional Feature Enginearing}
% References in APA format
\newpage
\bibliographystyle{apacite}
\bibliography{references}  % Ensure there is a file named references.bib with your bibliography entries.

\end{document}